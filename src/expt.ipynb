{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import torch as T\n",
    "import torch\n",
    "\n",
    "def normalize(mat):\n",
    "\t\"\"\"mat gets row-normalized\"\"\"\n",
    "\trow_sum = np.array(mat.sum(1)) + 1e-5\n",
    "\treciprocal = np.reciprocal(row_sum).flatten()\n",
    "\treciprocal[np.isinf(reciprocal)] = 0\n",
    "\treciprocal_mat = sparse.diags(reciprocal)\n",
    "\treturn reciprocal_mat.dot(mat)\n",
    "\n",
    "def get_sparse_tensor(mat):\n",
    "\tmat = mat.tocoo().astype(np.float32)\n",
    "\tindices = T.from_numpy(\n",
    "\t\tnp.vstack((mat.row, mat.col)).astype(np.int64)\n",
    "\t)\n",
    "\tvalues = T.from_numpy(mat.data)\n",
    "\treturn T.sparse.FloatTensor(indices, values, T.Size(mat.shape))\n",
    "\n",
    "def parse_data(dataset, verbose=True):\n",
    "\n",
    "    x = []\n",
    "    with open('../data/{}/{}.txt'.format(dataset, 'feature'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        row = line.strip().split('\\t')\n",
    "        row = [int(item) for item in row]\n",
    "        x.append(row)\n",
    "    x = T.from_numpy(normalize(np.array(x)))\n",
    "\n",
    "    y = []\n",
    "    with open('../data/{}/{}.txt'.format(dataset, 'group'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        row = line.strip().split('\\t')\n",
    "        row = int(row[1])\n",
    "        y.append(row)\n",
    "    y = np.array(y)\n",
    "\n",
    "    E = []\n",
    "    with open('../data/{}/{}.txt'.format(dataset, 'graph'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        row = line.strip().split('\\t')\n",
    "        u = int(row[0])\n",
    "        v = int(row[1])\n",
    "        E.append([u, v])\n",
    "    E = np.array(E)\n",
    "    A = sparse.coo_matrix(\n",
    "        (np.ones(E.shape[0]), (E[:, 0], E[:, 1])),\n",
    "        (len(y), len(y)),\n",
    "        np.float32\n",
    "    )\n",
    "    A += A.T.multiply(A.T > A) - A.multiply(A < A.T) #? logic\n",
    "    A = normalize(A + sparse.eye(len(y)))\n",
    "    A = get_sparse_tensor(A)\n",
    "    if verbose:\n",
    "        print(\"#Edges:\", len(lines))\n",
    "        print(\"#Classes:\", y.max() + 1)\n",
    "    \n",
    "    return x, A, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Edges: 5429\n",
      "#Classes: 7\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 2708])\n",
      "(2708,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 818, 1: 180, 2: 217, 3: 426, 4: 351, 5: 418, 6: 298}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, A, y = parse_data('cora')\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(x.shape)\n",
    "print(A.shape)\n",
    "print(y.shape)\n",
    "\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Edges: 4732\n",
      "#Classes: 6\n",
      "torch.Size([3312, 3703])\n",
      "torch.Size([3312, 3312])\n",
      "(3312,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 596, 1: 668, 2: 701, 3: 249, 4: 508, 5: 590}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, A, y = parse_data('citeseer')\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(x.shape)\n",
    "print(A.shape)\n",
    "print(y.shape)\n",
    "\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class PULoss(nn.Module):\n",
    "    def __init__(self, prior=torch.tensor(0.5), loss=nn.SoftMarginLoss(reduction='none'), beta=0, nnpu=True) -> None:\n",
    "        super().__init__()\n",
    "        self.prior = prior.cuda()\n",
    "        self.beta = beta\n",
    "        self.loss_func = loss\n",
    "        self.nnpu = nnpu\n",
    "        self.positive = 1\n",
    "        self.unlabelled = 0\n",
    "        self.min_count = torch.tensor(1.)\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        positive, unlabelled = target == self.positive, target == self.unlabelled\n",
    "        positive, unlabelled = positive.type(torch.float), unlabelled.type(torch.float)\n",
    "        n_pos, n_unlb = torch.max(self.min_count, torch.sum(positive)),\\\n",
    "             torch.max(self.min_count, torch.sum(unlabelled))\n",
    "        \n",
    "        y_pos = self.loss_func(inp, torch.ones_like(target)) * positive\n",
    "        # y_pos_inv = self.loss_func(inp, torch.zeros_like(target)) * positive\n",
    "        y_pos_inv = self.loss_func(inp, -torch.ones_like(target)) * positive\n",
    "\n",
    "        # y_unlabelled = self.loss_func(inp, torch.zeros_like(target)) * unlabelled\n",
    "        y_unlabelled = self.loss_func(inp, -torch.ones_like(target)) * unlabelled\n",
    "\n",
    "\n",
    "        positive_risk = self.prior * torch.sum(y_pos) / n_pos\n",
    "        negative_risk = torch.sum(y_unlabelled) / n_unlb - self.prior * torch.sum(y_pos_inv) / n_pos\n",
    "\n",
    "        if self.nnpu and negative_risk < -self.beta:\n",
    "            return positive_risk\n",
    "        else:\n",
    "            return positive_risk + negative_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.fc = nn.Linear(nclass, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training) #TODO: no dropout\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary(y, class_label, p):\n",
    "    mask = y == class_label\n",
    "    y_binary_test = mask.astype(int)\n",
    "    y_binary_train = np.zeros_like(y_binary_test)\n",
    "    P = np.nonzero(mask)[0]\n",
    "    N = np.nonzero(~mask)[0]\n",
    "    k = len(P)\n",
    "    N_equal = np.random.choice(N, k, False)\n",
    "    indices = np.concatenate((P, N_equal))\n",
    "    P_train = np.random.choice(P, int(k * p), False) #TODO: check ceil/floor\n",
    "    y_binary_train[P_train] = 1\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    indices = torch.from_numpy(indices)\n",
    "    y_binary_train = torch.from_numpy(y_binary_train)\n",
    "    y_binary_test = torch.from_numpy(y_binary_test)\n",
    "\n",
    "    return indices, y_binary_train, y_binary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "N_ITER = 10\n",
    "seeds = np.random.randint(1000, size=N_ITER) \n",
    "f1_scores_global = []\n",
    "\n",
    "\n",
    "def get_table(\n",
    "    nnpu = False,\n",
    "    p = 0.01,\n",
    "    dataset = 'cora',\n",
    "    class_label = 3,\n",
    "    verbose = False,\n",
    "    eps = 1e-2\n",
    "):\n",
    "    x_, A_, y_ = parse_data(dataset, False)\n",
    "    for _ in range(1):\n",
    "        f1_scores = []\n",
    "        for i in range(N_ITER):\n",
    "\n",
    "\n",
    "            np.random.seed(seeds[i])\n",
    "            model = GCN(nfeat=x_.shape[1],\n",
    "                nhid=64,\n",
    "                nclass=2).to(\"cuda\")\n",
    "\n",
    "            def train(epoch, verbose=False, nnpu=False):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x, A)\n",
    "                criterion = PULoss(nnpu=nnpu)\n",
    "                loss = criterion(output[indices].view(-1), y_binary_train[indices]) # !CHECK\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if verbose:\n",
    "                    print('Epoch: {}\\tLoss:{}'.format(epoch, loss.item()))\n",
    "                return loss.item()\n",
    "\n",
    "            def test(verbose=True, nnpu=False):\n",
    "                model.eval()\n",
    "                output = model(x, A)\n",
    "                criterion = PULoss(nnpu=nnpu)\n",
    "                loss = criterion(output[indices].view(-1), y_binary_test[indices])\n",
    "                pred = torch.where(output < 0.5, torch.tensor(0, device=device), \n",
    "                    torch.tensor(1, device=device))\n",
    "                f1 = f1_score(y_binary_test[indices].cpu(), pred[indices].cpu())\n",
    "                if verbose:\n",
    "                    print('f1:{}\\tLoss:{}'.format(f1, loss.item()))\n",
    "                return f1\n",
    "\n",
    "           \n",
    "            indices, y_binary_train, y_binary_test = make_binary(y_, class_label, p)\n",
    "            x = x_.cuda().float()\n",
    "            A = A_.cuda().float()\n",
    "            y = T.from_numpy(y_).cuda().float()\n",
    "            indices = indices.cuda()\n",
    "            y_binary_train = y_binary_train.cuda().float()\n",
    "            y_binary_test = y_binary_test.cuda().float()\n",
    "            device = torch.device(\"cuda\")\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "            num_epochs = 1000\n",
    "            epoch = 1\n",
    "            prev_loss = 1e10\n",
    "            while True:\n",
    "                cur_loss = train(epoch, False, nnpu)\n",
    "                if abs(prev_loss - cur_loss) < eps: break\n",
    "                prev_loss = cur_loss\n",
    "                epoch += 1\n",
    "\n",
    "            f1_scores.append(test(verbose, nnpu))  \n",
    "\n",
    "        print(p, np.mean(f1_scores),'±' ,np.std(f1_scores))\n",
    "        # f1_scores_global.append(np.mean(f1_scores))\n",
    "    # print(\"global\", np.mean(f1_scores_global),'±' ,np.std(f1_scores_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.2 ± 0.30550504633038933\n",
      "0.01 0.3333333333333333 ± 0.33333333333333337\n",
      "0.01 0.4666666666666667 ± 0.30550504633038933\n",
      "0.01 0.33604213694507146 ± 0.3307132906154237\n",
      "0.01 0.3333333333333333 ± 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 1/100,\n",
    "        dataset = 'cora',\n",
    "        class_label = 3,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 0.5333333333333334 ± 0.26666666666666666\n",
      "0.02 0.2668235294117647 ± 0.3267910315722148\n",
      "0.02 0.34412790788629716 ± 0.32357473512299917\n",
      "0.02 0.26666666666666666 ± 0.3265986323710904\n",
      "0.02 0.5333333333333334 ± 0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 2/100,\n",
    "        dataset = 'cora',\n",
    "        class_label = 3,\n",
    "        verbose = False,\n",
    "        eps=1e-4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.3333333333333333 ± 0.33333333333333337\n",
      "0.01 0.13298195176489377 ± 0.265965064107742\n",
      "0.01 0.39999999999999997 ± 0.3265986323710904\n",
      "0.01 0.36843082636954494 ± 0.31432354123919143\n",
      "0.01 0.3333333333333333 ± 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 1/100,\n",
    "        dataset = 'citeseer',\n",
    "        class_label = 2,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 0.26666666666666666 ± 0.3265986323710904\n",
      "0.02 0.3328130081300813 ± 0.3328162620532997\n",
      "0.02 0.13333333333333333 ± 0.26666666666666666\n",
      "0.02 0.32717086834733894 ± 0.327634834675406\n",
      "0.02 0.39919523579591176 ± 0.32594982420607405\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 2/100,\n",
    "        dataset = 'citeseer',\n",
    "        class_label = 2,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03 0.5317734825364531 ± 0.26592677396077746\n",
      "0.03 0.3333333333333333 ± 0.33333333333333337\n",
      "0.03 0.4666666666666666 ± 0.30550504633038933\n",
      "0.03 0.2 ± 0.30550504633038933\n",
      "0.03 0.26666666666666666 ± 0.3265986323710904\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 3/100,\n",
    "        dataset = 'citeseer',\n",
    "        class_label = 2,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04 0.39999999999999997 ± 0.3265986323710904\n",
      "0.04 0.3333333333333333 ± 0.33333333333333337\n",
      "0.04 0.2666032350142721 ± 0.3265209909897398\n",
      "0.04 0.3333333333333333 ± 0.33333333333333337\n",
      "0.04 0.2 ± 0.30550504633038933\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 4/100,\n",
    "        dataset = 'citeseer',\n",
    "        class_label = 2,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 0.26666666666666666 ± 0.3265986323710904\n",
      "0.05 0.39999999999999997 ± 0.3265986323710904\n",
      "0.05 0.26666666666666666 ± 0.3265986323710904\n",
      "0.05 0.32938304482911673 ± 0.32957249347455\n",
      "0.05 0.3333333333333333 ± 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = False,\n",
    "        p = 5/100,\n",
    "        dataset = 'citeseer',\n",
    "        class_label = 2,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.4 ± 0.3265986323710904\n",
      "0.01 0.3333333333333333 ± 0.33333333333333337\n",
      "0.01 0.3333333333333333 ± 0.33333333333333337\n",
      "0.01 0.3331427057548713 ± 0.33314299037944195\n",
      "0.01 0.4666666666666666 ± 0.30550504633038933\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_table(\n",
    "        nnpu = True,\n",
    "        p = 1/100,\n",
    "        dataset = 'citeseer',\n",
    "        class_label = 2,\n",
    "        verbose = False,\n",
    "        eps=1e-3\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerType(enum.Enum):\n",
    "    IMP1 = 0,\n",
    "    IMP2 = 1,\n",
    "    IMP3 = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    I've added 3 GAT implementations - some are conceptually easier to understand some are more efficient.\n",
    "\n",
    "    The most interesting and hardest one to understand is implementation #3.\n",
    "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
    "\n",
    "    Tip on how to approach this:\n",
    "        understand implementation 2 first, check out the differences it has with imp1, and finally tackle imp #3.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
    "                 dropout=0.6, layer_type=LayerType.IMP3, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
    "\n",
    "        GATLayer = get_layer_type(layer_type)  # fetch one of 3 available implementations\n",
    "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
    "\n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias,\n",
    "                log_attention_weights=log_attention_weights\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.gat_net = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "\n",
    "    # data is just a (in_nodes_features, topology) tuple, I had to do it like this because of the nn.Sequential:\n",
    "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
    "    def forward(self, data):\n",
    "        return self.gat_net(data)\n",
    "\n",
    "\n",
    "class GATLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for all implementations as there is much code that would otherwise be copy/pasted.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    head_dim = 1\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, layer_type, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving these as we'll need them in forward propagation in children layers (imp1/2/3)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        if layer_type == LayerType.IMP1:\n",
    "            # Experimenting with different options to see what is faster (tip: focus on 1 implementation at a time)\n",
    "            self.proj_param = nn.Parameter(torch.Tensor(num_of_heads, num_in_features, num_out_features))\n",
    "        else:\n",
    "            # You can treat this one matrix as num_of_heads independent W matrices\n",
    "            self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the additive scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        if layer_type == LayerType.IMP1:  # simple reshape in the case of implementation 1\n",
    "            self.scoring_fn_target = nn.Parameter(self.scoring_fn_target.reshape(num_of_heads, num_out_features, 1))\n",
    "            self.scoring_fn_source = nn.Parameter(self.scoring_fn_source.reshape(num_of_heads, num_out_features, 1))\n",
    "\n",
    "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
    "        self.softmax = nn.Softmax(dim=-1)  # -1 stands for apply the log-softmax along the last dimension\n",
    "        self.activation = activation\n",
    "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
    "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
    "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
    "\n",
    "        self.init_params(layer_type)\n",
    "\n",
    "    def init_params(self, layer_type):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.proj_param if layer_type == LayerType.IMP1 else self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
    "            self.attention_weights = attention_coefficients\n",
    "\n",
    "        # if the tensor is not contiguously stored in memory we'll get an error after we try to do certain ops like view\n",
    "        # only imp1 will enter this one\n",
    "        if not out_nodes_features.is_contiguous():\n",
    "            out_nodes_features = out_nodes_features.contiguous()\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
    "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
    "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)\n",
    "\n",
    "\n",
    "class GATLayerImp3(GATLayer):\n",
    "    \"\"\"\n",
    "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
    "\n",
    "    But, it's hopefully much more readable! (and of similar performance)\n",
    "\n",
    "    It's suitable for both transductive and inductive settings. In the inductive setting we just merge the graphs\n",
    "    into a single graph with multiple components and this layer is agnostic to that fact! <3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        # Delegate initialization to the base class\n",
    "        super().__init__(num_in_features, num_out_features, num_of_heads, LayerType.IMP3, concat, activation, dropout_prob,\n",
    "                      add_skip_connection, bias, log_attention_weights)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
    "        # Note: for Cora features are already super sparse so it's questionable how much this actually helps\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (E, NH, 1)\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        # Add stochasticity to neighborhood aggregation\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    #\n",
    "    # Helper functions (without comments there is very little code so don't be scared!)\n",
    "    #\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \"\"\"\n",
    "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
    "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
    "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
    "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
    "        (where 1-3 is overloaded notation it represents the edge 1-3 and it's (exp) score) and similarly for 2-3 and 3-3\n",
    "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
    "\n",
    "        Note:\n",
    "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
    "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
    "        Check out this link for more details:\n",
    "\n",
    "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        # Calculate the denominator. shape = (E, NH)\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
    "        # possibility of the computer rounding a very small number all the way to 0.\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "\n",
    "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "\n",
    "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
    "        # target index)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
    "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "\n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "\n",
    "\n",
    "class GATLayerImp2(GATLayer):\n",
    "    \"\"\"\n",
    "        Implementation #2 was inspired by the official GAT implementation: https://github.com/PetarV-/GAT\n",
    "\n",
    "        It's conceptually simpler than implementation #3 but computationally much less efficient.\n",
    "\n",
    "        Note: this is the naive implementation not the sparse one and it's only suitable for a transductive setting.\n",
    "        It would be fairly easy to make it work in the inductive setting as well but the purpose of this layer\n",
    "        is more educational since it's way less efficient than implementation 3.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__(num_in_features, num_out_features, num_of_heads, LayerType.IMP2, concat, activation, dropout_prob,\n",
    "                         add_skip_connection, bias, log_attention_weights)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization (using linear layer instead of matmul as in imp1)\n",
    "        #\n",
    "\n",
    "        in_nodes_features, connectivity_mask = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[0]\n",
    "        assert connectivity_mask.shape == (num_of_nodes, num_of_nodes), \\\n",
    "            f'Expected connectivity matrix with shape=({num_of_nodes},{num_of_nodes}), got shape={connectivity_mask.shape}.'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation (using sum instead of bmm + additional permute calls - compared to imp1)\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1)\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = torch.sum((nodes_features_proj * self.scoring_fn_source), dim=-1, keepdim=True)\n",
    "        scores_target = torch.sum((nodes_features_proj * self.scoring_fn_target), dim=-1, keepdim=True)\n",
    "\n",
    "        # src shape = (NH, N, 1) and trg shape = (NH, 1, N)\n",
    "        scores_source = scores_source.transpose(0, 1)\n",
    "        scores_target = scores_target.permute(1, 2, 0)\n",
    "\n",
    "        # shape = (NH, N, 1) + (NH, 1, N) -> (NH, N, N) with the magic of automatic broadcast <3\n",
    "        # In Implementation 3 we are much smarter and don't have to calculate all NxN scores! (only E!)\n",
    "        # Tip: it's conceptually easier to understand what happens here if you delete the NH dimension\n",
    "        all_scores = self.leakyReLU(scores_source + scores_target)\n",
    "        # connectivity mask will put -inf on all locations where there are no edges, after applying the softmax\n",
    "        # this will result in attention scores being computed only for existing edges\n",
    "        all_attention_coefficients = self.softmax(all_scores + connectivity_mask)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation (same as in imp1)\n",
    "        #\n",
    "\n",
    "        # batch matrix multiply, shape = (NH, N, N) * (NH, N, FOUT) -> (NH, N, FOUT)\n",
    "        out_nodes_features = torch.bmm(all_attention_coefficients, nodes_features_proj.transpose(0, 1))\n",
    "\n",
    "        # Note: watch out here I made a silly mistake of using reshape instead of permute thinking it will\n",
    "        # end up doing the same thing, but it didn't! The acc on Cora didn't go above 52%! (compared to reported ~82%)\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = out_nodes_features.permute(1, 0, 2)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias (same as in imp1)\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(all_attention_coefficients, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, connectivity_mask)\n",
    "\n",
    "\n",
    "class GATLayerImp1(GATLayer):\n",
    "    \"\"\"\n",
    "        This implementation is only suitable for a transductive setting.\n",
    "        It would be fairly easy to make it work in the inductive setting as well but the purpose of this layer\n",
    "        is more educational since it's way less efficient than implementation 3.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__(num_in_features, num_out_features, num_of_heads, LayerType.IMP1, concat, activation, dropout_prob,\n",
    "                         add_skip_connection, bias, log_attention_weights)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, connectivity_mask = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[0]\n",
    "        assert connectivity_mask.shape == (num_of_nodes, num_of_nodes), \\\n",
    "            f'Expected connectivity matrix with shape=({num_of_nodes},{num_of_nodes}), got shape={connectivity_mask.shape}.'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN number of input features per node\n",
    "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # shape = (1, N, FIN) * (NH, FIN, FOUT) -> (NH, N, FOUT) where NH - number of heads, FOUT num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = torch.matmul(in_nodes_features.unsqueeze(0), self.proj_param)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # batch matrix multiply, shape = (NH, N, FOUT) * (NH, FOUT, 1) -> (NH, N, 1)\n",
    "        scores_source = torch.bmm(nodes_features_proj, self.scoring_fn_source)\n",
    "        scores_target = torch.bmm(nodes_features_proj, self.scoring_fn_target)\n",
    "\n",
    "        # shape = (NH, N, 1) + (NH, 1, N) -> (NH, N, N) with the magic of automatic broadcast <3\n",
    "        # In Implementation 3 we are much smarter and don't have to calculate all NxN scores! (only E!)\n",
    "        # Tip: it's conceptually easier to understand what happens here if you delete the NH dimension\n",
    "        all_scores = self.leakyReLU(scores_source + scores_target.transpose(1, 2))\n",
    "        # connectivity mask will put -inf on all locations where there are no edges, after applying the softmax\n",
    "        # this will result in attention scores being computed only for existing edges\n",
    "        all_attention_coefficients = self.softmax(all_scores + connectivity_mask)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # shape = (NH, N, N) * (NH, N, FOUT) -> (NH, N, FOUT)\n",
    "        out_nodes_features = torch.bmm(all_attention_coefficients, nodes_features_proj)\n",
    "\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = out_nodes_features.transpose(0, 1)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias (same across all the implementations)\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(all_attention_coefficients, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, connectivity_mask)\n",
    "\n",
    "\n",
    "#\n",
    "# Helper functions\n",
    "#\n",
    "def get_layer_type(layer_type):\n",
    "    assert isinstance(layer_type, LayerType), f'Expected {LayerType} got {type(layer_type)}.'\n",
    "\n",
    "    if layer_type == LayerType.IMP1:\n",
    "        return GATLayerImp1\n",
    "    elif layer_type == LayerType.IMP2:\n",
    "        return GATLayerImp2\n",
    "    elif layer_type == LayerType.IMP3:\n",
    "        return GATLayerImp3\n",
    "    else:\n",
    "        raise Exception(f'Layer type {layer_type} not yet supported.')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('g')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8349d0dad13e6de547ca0886be80c2b100bc06b16167523118758b1780f61209"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
